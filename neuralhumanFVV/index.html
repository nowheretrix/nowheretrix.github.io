<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering using RGB Cameras">
    <meta name="author" content="Yuheng Jiang,
                                Suyi Jiang,
                                Guoxing Sun,
                                Zhuo Su,
                                Kaiwen Guo,
                                Minye Wu,
                                Jingyi Yu, 
                                Lan Xu">

    <title>NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering using RGB Cameras</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering<br> using RGB Cameras</h2>
        <h3>CVPR 2021</h3>
           <p class="abstract">4D reconstruction and rendering of human activities using 6 rgb cameras.</p>
    <hr>
    <p class="authors">
        <a > Xin Suo</a>,
        <a href="https://nowheretrix.github.io/"> Yuheng Jiang</a>,
        <a > Pei Lin</a>,
        <a > Yingliang Zhang</a>, </br>
        <a href="https://wuminye.com/"> Minye Wu</a>,
        <a > Kaiwen Guo</a>,
        <a href="http://xu-lan.com/"> Lan Xu</a>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://openaccess.thecvf.com/content/CVPR2021/html/Suo_NeuralHumanFVV_Real-Time_Neural_Volumetric_Human_Performance_Rendering_Using_RGB_Cameras_CVPR_2021_paper.html">Paper</a>
        <!-- <a class="btn btn-primary" href="https://github.com/apchenstu/mvsnerf">Code</a> -->
    </div>
</div>

<div class="container">
    <div class="section">
        <div class="vcontainer">
            <iframe class='video' src="https://www.youtube.com/embed/45g2Pxg7Ot4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
        <hr>
        <p>
            4D reconstruction and rendering of human activities is critical for immersive VR/AR experience. Recent advances still fail to recover fine geometry and texture results with the level of detail present in the input images from sparse multi-view RGB cameras. In this paper, we propose NeuralHumanFVV, a real-time neural human performance capture and rendering system to generate both high-quality  geometry and photo-realistic texture of human activities in arbitrary novel views. We propose a neural geometry generation scheme with a hierarchical sampling strategy for real-time implicit geometry inference, as well as a novel neural blending scheme to generate high resolution (e.g., 1k) and photo-realistic texture results in the novel views. Furthermore, we adopt neural normal blending to enhance geometry details and formulate our neural geometry and texture rendering into a multi-task learning framework. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality geometry and photo-realistic free view-point reconstruction for challenging human performances.
        </p>
    </div>

<!-- 
    <div class="section">
        <h2>Pipeline</h2>
        <hr>
        <p>
            Our framework first constructs a <b>cost volume</b> (a) by warping 2D image features onto a plane sweep. We then apply 3D CNN to reconstruct a <b>neural encoding volume</b> with per-voxel neural features (b). We use an MLP to regress volume density and RGB radiance at an arbitrary location using features interpolated from the encoding volume. These volume properties are used by differentiable ray marching for final rendering (c).
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/pipeline.png" style="width:100%; margin-right:-10px; margin-top:-10px;">
        </div> 
    </div>

    <div class="section">
        <h2>Result</h2>
        <hr>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/teaser.png" style="width:100%; margin-right:-10px; margin-top:-10px;">
        </div>
        <p>
            We train our MVSNeRF with scenes of objects in the DTU dataset. Our network can effectively <b>generalize</b> across diverse scenes; even for a complex indoor scene, our network can reconstruct a neural radiance field from only three input images (a) and synthesize a realistic image from a novel viewpoint (b). While this result contains artifacts, it can be largely improved by fine-tuning our reconstruction on more images for only <b>6 min</b> (c), which achieves better quality than the NeRF's nerf result (d) from 9.5h per-scene optimization. 
        </p>
    </div>

    <div class="section">
        <h2>Results</h2>
        <hr>

        <div class="col justify-content-center text-center">
            <img src="img/progress.png" style="width:100%; margin-right:-10px; margin-top:10px;">

        <p>
            Optimization progress. We show results of our fine-tuning (top) and optimizing a <b>NeRF</b> (bottom) with different time periods. Our <b>0-min</b> result refers to the initial output from our network inference. Note that our <b>18-min</b> results are already much better than the <b>215-min</b> NeRF results. PSNRs of the image crops are shown in the figure.
        </p>

        <div class="col justify-content-center text-center">
            <img src="img/result.png" style="width:100%; margin-right:-10px; margin-top:10px;">

        <p>
            Rendering quality comparison. On the left, we show rendering results of our method and concurrent neural rendering methods PixelNeRF, IBRNet by directly running the networks. We show our 15-min fine-tuning results and NeRF's  10.2h-optimization results on the right.
        </p>
    </div>

    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href="https://arxiv.org/abs/2103.15595"
                   class="list-group-item">
                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div> -->

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @inproceedings{suo2021neuralhumanfvv,
                title={NeuralHumanFVV: Real-time neural volumetric human performance rendering 
                       using RGB cameras},
                author={Suo, Xin and Jiang, Yuheng and Lin, Pei and Zhang, Yingliang and Wu, Minye 
                        and Guo, Kaiwen and Xu, Lan},
                booktitle={Proceedings of the IEEE/CVF conference on computer vision 
                           and pattern recognition},
                pages={6226--6237},
                year={2021}
              }
        </div>
    </div>

    <hr>

    <footer>
        <p>Send feedback and questions to <a href="https://nowheretrix.github.io/">Yuheng Jiang</a></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
